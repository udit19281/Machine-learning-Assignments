# -*- coding: utf-8 -*-
"""ML_A2_Q1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1hdgt9m7wv9NqMunjKQNcS7v8yEAByElC
"""

# Commented out IPython magic to ensure Python compatibility.
from google.colab import drive
drive.mount('/content/gdrive')
# %cd '/content/gdrive/MyDrive/ML datasets/ML A2/'
# %ls

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.tree import DecisionTreeClassifier

#Set random seed
np.random.seed(0)

#Read csv data
data=pd.read_csv('PRSA_data_2010.1.1-2014.12.31.csv')

#Drop No column
data=data.drop(['No'],axis=1)

#Shuffle sample
data=data.sample(frac=1)

#Data describe
print(data.describe())

print(data.isna().sum())   #pm2.5 has 2067 NA values

data['pm2.5']=data['pm2.5'].fillna(data['pm2.5'].median())

#print(data.isna().sum())   #NO Na values now

#Print counts of cbwd column
print(data['cbwd'].value_counts())

#Encord values
cbwd={
'SE':0,
'NW':1,
'cv':2,
'NE':3,
}
#Label column values
for i in range(len(data['cbwd'])):
  data.iloc[i,8]=cbwd[data.iloc[i,8]]

print(data.head(10),"\n",data.shape)
rows=data.shape[0]
#convert to numpy
data_numpy=data.values

#Train,valid, test split
train=data_numpy[0:int(rows*0.70),:]
valid=data_numpy[int(rows*(0.70)):int(rows*(0.85)),:]
test=data_numpy[int(rows*(0.85)):,]

print(train.shape,valid.shape,test.shape)

#Calculate score
def calculate_score(ypred,y):
  acc=((ypred==y).sum()/len(y))
  return acc

#Feature and output vectors
train_x, train_y=np.delete(train,1,axis=1).astype('int'), train[:,1].astype('int')
test_x, test_y  =np.delete(test,1,axis=1).astype('int'), test[:,1].astype('int')
valid_x, valid_y=np.delete(valid,1,axis=1).astype('int'), valid[:,1].astype('int')

#Part a

#Model using gini
model1=DecisionTreeClassifier(criterion='gini')
model1.fit(train_x,train_y)
ypred1=model1.predict(test_x)
accuracy=calculate_score(ypred1,test_y)
print("Using Gini Index:","\nModel Accuracy:",accuracy)

#Model using entropy
model2=DecisionTreeClassifier(criterion='entropy')
model2.fit(train_x,train_y)
ypred2=model2.predict(test_x)
accuracy=calculate_score(ypred2,test_y)
print("Using Entropy:","\nModel Accuracy:",accuracy)

#Part b
depths=[2,4,8,10,15,30]

training_accuracy=[]  #Training accuracy list
testing_accuracy=[] #Testing accuracy list

for i in depths:
  model=DecisionTreeClassifier(criterion='entropy',max_depth=i)
  model.fit(train_x,train_y)
  training_accuracy.append(calculate_score(model.predict(train_x),train_y))
  testing_accuracy.append(calculate_score(model.predict(test_x),test_y))

#Plot graph
plt.figure(figsize=(7,7))
plt.plot(depths,training_accuracy,label='Training Accuracy')
plt.plot(depths,testing_accuracy,label='Testing Accuracy')
plt.xlabel('Depths')
plt.ylabel('Accuracy')
plt.legend()
plt.show()
#Best value at max depth=15

#Part c

#Return 50% random data
def randomData():
  train_temp=np.copy(train)
  i= np.random.randint(1,10,1)[0]
  rows=train_temp.shape[0]
  index=np.random.choice(rows,size=int(0.5*(rows)),replace=False)
  train_temp=train_temp[index,:]
  random_train_x, random_train_y=np.delete(train_temp,1,axis=1).astype('int'), train_temp[:,1].astype('int')
  return random_train_x, random_train_y

#Predict results using maximum vote count and return accuracy  
def predict_and_accuracy(model_list,x,y):
  result=np.zeros(y.shape)
  for model in model_list:
    result=np.vstack( ( result, np.array(model.predict(x)) ) )
  results=result[1:,:]
  #Taking the majority voting
  predict, ind = np.unique(results, return_inverse=True)
  predict=predict[np.argmax(np.apply_along_axis(np.bincount, 0, ind.reshape(results.shape),
                                None, np.max(ind) + 1), axis=0)]
  return ((predict==y).sum()/len(y))

#Implements ensembling function
def ensembling(model_numbers=100,depth=3):
  model_list=[]
  for i in range(model_numbers):
    temp_model=DecisionTreeClassifier(criterion='entropy',max_depth=depth)
    random_train_x, random_train_y=randomData()
    temp_model.fit(random_train_x, random_train_y)
    model_list.append(temp_model)
  accuracy={
      'training_accuracy':predict_and_accuracy(model_list,train_x,train_y),
       'testing_accuracy':predict_and_accuracy(model_list,test_x,test_y),
       'validation_accuracy':predict_and_accuracy(model_list,valid_x,valid_y)
  }
  return accuracy

#Ensembling for stumps depth=3 and stump count=100
accuracy=ensembling()
print("Accuracy:")
for i,j in accuracy.items():
  print(i,':',j)

#Part d
depths=[4,8,10,15,20]
model_numbers=[100,150,200]

#Change depth, number of trees and print results
for i in model_numbers:
  for j in depths:
    acc=ensembling(i,j)
    print(f"Results for max_depth={j} and total stumps={i}")
    for key,value in acc.items():
      print(key,':',value)
    print('\n')

# Best result at , max_depth=200 and total strumps=100

#Part e
from sklearn.ensemble import AdaBoostClassifier

#Running ada boost on our dataset
model=AdaBoostClassifier(base_estimator=DecisionTreeClassifier(criterion='entropy'))
model.fit(train_x,train_y)
ypred=model.predict(test_x)
accuracy=calculate_score(ypred,test_y)
print("Using AdaBoost:","\nModel Accuracy:",accuracy,'\n')

estimators=[4,8,10,15,20]
#Run the ada boost classifer for different estimators and print results
for i in estimators:
  model=AdaBoostClassifier(base_estimator=DecisionTreeClassifier(criterion='entropy'),n_estimators=i)
  random_train_x, random_train_y=randomData()
  model.fit(random_train_x, random_train_y)
  acc={
      'training_accuracy':calculate_score( model.predict(train_x),train_y),
       'testing_accuracy':calculate_score( model.predict(test_x),test_y),
       'validation_accuracy':calculate_score( model.predict(valid_x),valid_y)
  }
  print(f'Results for estimator={i}')
  for key,value in acc.items():
      print(key,':',value)
  print('\n')