# -*- coding: utf-8 -*-
"""ML_A2_Q2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Uj_sc9jUzNaXO4KP59rb38CLaZlwN9um
"""

# Commented out IPython magic to ensure Python compatibility.
from google.colab import drive
drive.mount('/content/gdrive')
# %cd '/content/gdrive/MyDrive/ML datasets/ML A2/'
# %ls

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler

np.random.seed(0)
d1=pd.read_csv('train.csv')
d2=pd.read_csv('test.csv')
data=pd.concat([d1,d2],axis=0)

# data=data.sample(frac=1)
scale=StandardScaler()

data_numpy=data.values
rows=data_numpy.shape[0]

data_x=data.drop('label',axis=1)
data_x=data_x.values
data_x=data_x/255

data_y=data['label']
data_y=data_y.values

train_x=data_x[0:int(rows*0.70),:]
test_x=data_x[int(rows*(0.70)):int(rows*(0.90)),:]
valid_x=data_x[int(rows*(0.90)):,]

train_y=data_y[0:int(rows*0.70)]
test_y=data_y[int(rows*(0.70)):int(rows*(0.90))]
valid_y=data_y[int(rows*(0.90)):]

train_x=scale.fit_transform(train_x)
test_x=scale.transform(test_x)
valid_x=scale.transform(valid_x)

print(train_x.shape,train_y.shape,test_x.shape,test_y.shape,valid_x.shape,valid_y.shape)

import math
#Class neural network
class MyNeuralNetwork:
  #init
  def __init__(self,Layer_sizes,learning_rate,batch_size,epochs,weight_init='random',activation='relu'):
    self.activation=activation
    self.learningRate=learning_rate
    self.nLayers=len(Layer_sizes)
    self.layerSizes=Layer_sizes
    self.weightInit=weight_init
    self.batchSize=batch_size
    self.epochs=epochs
    self.networkVariables={}
    self.loss_in_train=[]
    self.loss_in_val=[]
    self.acc_in_train=[]
    self.acc_in_val=[]
    self.parameters=self.networkVariables

    if (self.activation not in ['relu','sigmoid','leakyrelu','linear','tanh','softmax']) or (self.weightInit not in ['random','zero','normal']):
      raise Exception('Error, check activation or weight init')
  
  #init params
  def init(self):
    i=0
    networkVariables={}
    while i in range(0,len(self.layerSizes)-1):
      if self.weightInit=='random':
        w=self.weight_init_random(self.layerSizes[i],self.layerSizes[i+1])
        b=np.zeros((1,self.layerSizes[i+1]))
      
      elif self.weightInit=='normal':
        w=self.weight_init_normal(self.layerSizes[i],self.layerSizes[i+1])
        b=np.zeros((1,self.layerSizes[i+1]))

      elif self.weightInit=='zero':
        w=self.weight_init_zero(self.layerSizes[i],self.layerSizes[i+1])
        b=np.zeros((1,self.layerSizes[i+1]))

      networkVariables["W"+str(i+1)]=w
      networkVariables["b"+str(i+1)]=b
      i=i+1
    self.networkVariables=networkVariables
    return networkVariables

  #Random Weigth init
  def weight_init_random(self,x1,y1):
    weights=np.random.rand(x1,y1)*0.01
    # print(weights)
    return weights

  #zero Weigth init
  def weight_init_zero(self,x1,y1):
    weights=np.zeros((x1,y1))
    return weights

  #Normal Weigth init
  def weight_init_normal(self,x1,y1):
    sz=(x1,y1)
    weights=np.random.normal(size=sz,scale=0.01)
    return weights

  #Relu Function Defination
  def relu(self,y):
    return np.maximum(0,y)

  #Relu Function Derivative
  def reluDerivative(self,y):
    z=y>=0
    z=np.dot(z,1)
    return z

  #Leaky Relu Function Defination
  def leakyrelu(self,y):
    return np.where(y > 0, y, y * 0.01)  

  #Leaky Relu Function Derivative
  def leakyreluDerivative(self,y):
    return np.where(y > 0, 1, 0.01)

  #Sigmoid Function
  def sigmoid(self,z):
    return 1.0/(1.0+np.exp(-z))
  
  #Derivative of Sigmoid Function
  def sigmoidDerivative(self,z):
    k=self.sigmoid(z)
    return k*(1-k)

  #Softmax Function
  def softmax(self,z):
    ex = np.exp(z-np.max(z))
    return ex / np.sum(ex,axis=1,keepdims=True)

  #Derivative of Softmax Function
  def softmaxDerivative(self,z):
    der=self.softmax(z)
    return der*(1-der)

  #tanh Function
  def tanh(self,z):
    return np.tanh(z)

  #Derivative of tanh Function
  def tanhDerivative(self,z):
    temp=self.tanh(z)
    return 1-(temp*temp)

  #linear Function
  def linear(self,z):
    return z

  #Derivative of linear Function
  def linearDerivative(self,z):
    return np.ones(z.shape)

  #Categories each y value into 10 row values of 0's and 1's
  def categories(self,y):
        result = np.zeros((len(y),np.max(y)+1))
        for row in range(len(y)):
            col = y[row]
            result[row,col] = 1
        return result

  #Use given activation function
  def classifyActivation(self,z):
      if self.activation=='sigmoid':
        return self.sigmoid(z)
      elif self.activation=='relu':
        return self.relu(z)
      elif self.activation=='leakyrelu':
        return self.leakyrelu(z)
      elif self.activation=='linear':
        return self.linear(z)
      elif self.activation=='tanh':
        return self.tanh(z)
      elif self.activation=='softmax':
        return self.softmax(z)
      else:
        raise Exception('Error in Activation function') 
      
  #Derivative of Activation functions
  def calculateDerivative(self,z):
      if self.activation=='sigmoid':
        return self.sigmoidDerivative(z)
      elif self.activation=='relu':
        return self.reluDerivative(z)
      elif self.activation=='leakyrelu':
        return self.leakyreluDerivative(z)
      elif self.activation=='linear':
        return self.linearDerivative(z)
      elif self.activation=='tanh':
        return self.tanhDerivative(z)
      elif self.activation=='softmax':
        return self.softmaxDerivative(z)
      else:
        raise Exception('Error in Derivative')
  
  #Implements the forward propagation
  def forwardPropagation(self,x,networkVariables):
      activations_memory={}
      z_memory={}
      curr_activation=x
      half_net_size=int(len(networkVariables)/2)
      for i in range(0,half_net_size-1):
        prev_activation=curr_activation
        #Get the weights and biases
        curr_w=networkVariables["W"+str(i+1)]
        curr_b=networkVariables["b"+str(i+1)]
        #Current value of z
        z_curr=np.dot(prev_activation,curr_w)+curr_b
        #Current activation 
        curr_activation=self.classifyActivation(z_curr)
        prev_activation=curr_activation
        activations_memory["A"+str(i+1)]=curr_activation
        z_memory["z"+str(i+1)]=z_curr
      #Get the weights and biases for last layer
      curr_w=networkVariables["W"+str(half_net_size)]
      curr_b=networkVariables["b"+str(half_net_size)]
      #z for output layer
      z_output=np.dot(prev_activation,curr_w)+curr_b
      #Activation for output layer
      output_activation=self.softmax(z_output)
      activations_memory["A"+str(half_net_size)]=output_activation
      #update memory
      z_memory["z"+str(half_net_size)]=z_output
      return activations_memory,z_memory, z_output, output_activation

  #Update weights and biases
  def update(self,derivative_memory,sizze):
      index=0
      while index<sizze:
        self.networkVariables["W"+str(index+1)]=self.networkVariables["W"+str(index+1)]- self.learningRate*derivative_memory["deltaW"+str(index+1)]
        self.networkVariables["b"+str(index+1)]=self.networkVariables["b"+str(index+1)]- self.learningRate*derivative_memory["deltaB"+str(index+1)]
        index=index+1
      return

  #Implements the back propagation 
  def backwardPropagation(self,x,y,activations_memory,z_memory):
      sizze=len(activations_memory)
      activations_memory["A0"]=x
      act=activations_memory["A"+str(sizze)]
      #Calculation for last layer
      deltaZ=act-y
      deltaW=np.dot(activations_memory["A"+str(sizze-1)].T,deltaZ)/len(x)
      deltaB=np.sum(deltaZ,axis=0,keepdims=True)/len(x)
      prevDA=np.dot(deltaZ,self.networkVariables["W"+str(sizze)].T)
      #Store derivatives in dict
      derivative_memory={}
      derivative_memory["deltaB"+str(sizze)]=deltaB
      derivative_memory["deltaW"+str(sizze)]=deltaW

      #Repeat steps for backward layers
      for index in range(sizze-1,0,-1):
        act_d=self.calculateDerivative(z_memory["z"+str(index)])
        deltaZ=act_d*prevDA
        deltaW=1/(len(x))*np.dot(activations_memory["A"+str(index-1)].T,deltaZ)
        deltaB=np.sum(deltaZ,axis=0,keepdims=True)*(1/len(x))

        if not index<=1:
          prevDA= np.dot(deltaZ,self.networkVariables["W"+str(index)].T)
        derivative_memory["deltaB"+str(index)]=deltaB
        derivative_memory["deltaW"+str(index)]=deltaW
      #Update weights and biases
      self.update(derivative_memory,sizze)
      return

    #Cross Entropy Loss function
  def crossEntropyLoss(self,ypred,y):
      den=len(y)
      logg=np.log(ypred[np.arange(den),y.argmax(axis=1)]+1e-9)
      if den==0:
        den=1
      return np.sum(-logg)/den

  #Implements the fit function to train the model
  def fit(self,x_train, y_train,x_val,y_val):
      networkVariables=self.init()
      self.networkVariables=networkVariables
      rows_x,cols_x=x_train.shape
      y_train=self.categories(y_train)
      rows_y,cols_y=y_train.shape

      for iterate in range(0,self.epochs):
        
        total_batches=int(rows_x/self.batchSize)
        inputBatch=[]
        labelBatch=[]
        batchTrainLoss=[]
        batchValidLoss=[]
        # batchTrainAccuracy=[]
        # batchValidAccuracy=[]

        for index in range(0,total_batches):
          inputBatch.append(x_train[self.batchSize*index:self.batchSize*(index+1),:])
          labelBatch.append(y_train[self.batchSize*index:self.batchSize*(index+1),:])

        for i,j in zip(inputBatch,labelBatch):
          #Forward pass
          activations_memory,z_memory, z_output, output_activation=self.forwardPropagation(i,networkVariables)
          #Loss calculation
          loss=self.crossEntropyLoss(output_activation,j)
          validloss=self.crossEntropyLoss(self.predict_proba(x_val),self.categories(y_val))
          if not math.isnan(loss):
            batchTrainLoss.append(loss)
          else:
            batchTrainLoss.append(0)
          if not math.isnan(validloss):
            batchValidLoss.append(validloss)
          else:
            batchValidLoss.append(0)
          #Backward pass
          self.backwardPropagation(i,j,activations_memory,z_memory)
          # batchTrainAccuracy.append(self.score(i,np.argmax(j,axis=1)))
          # batchValidAccuracy.append(self.score(x_val,y_val))

        #Accuracy and loss update
        # self.acc_in_train.append(np.array(batchTrainAccuracy).mean())
        # self.acc_in_val.append(np.array(batchValidAccuracy).mean())
        lossm1=np.array(batchTrainLoss).mean()
        lossm2=np.array(batchValidLoss).mean()
        if iterate%40==0:
          print(f"Epochs {iterate+1}, train loss={lossm1}, valid loss={lossm2}")
        #Loss list update
        self.loss_in_train.append(lossm1)
        self.loss_in_val.append(lossm2)
        self.parameters=self.networkVariables
      return 
  
  #Return the class wise Probability
  def predict_proba(self,x):
    _,_, _, output=self.forwardPropagation(x,self.networkVariables)
    return output

  #Prediction using trained model
  def predict(self,x):
    return np.argmax(self.predict_proba(x),axis=1)

  #Calculate Score
  def score(self,x_test,y_test):
    ypred=self.predict(x_test)
    return (ypred==y_test).sum()/len(y_test)

#Calculate score
def calculate_score(ypred,y):
  acc=((ypred==y).sum()/len(y))
  return acc

np.seterr(divide='ignore', invalid='ignore',over='ignore')

#Define models
model_relu=MyNeuralNetwork([784, 256, 128, 64,32, 10], 0.08,len(train_x)//35,150, 'normal','relu')
model_leakyrelu=MyNeuralNetwork([784, 256, 128, 64,32, 10], 0.08,len(train_x)//35,150, 'normal','leakyrelu')
model_sigmoid=MyNeuralNetwork([784, 256, 128, 64, 32,10], 0.08,len(train_x)//35,150, 'normal','sigmoid')
model_linear=MyNeuralNetwork([784, 256, 128, 64, 32,10], 0.08,len(train_x)//35,150, 'normal','linear')
model_tanh=MyNeuralNetwork([784, 256, 128, 64, 32,10], 0.08,len(train_x)//35,150, 'normal','tanh')
model_softmax=MyNeuralNetwork([784, 256, 128, 64, 32,10], 0.08,len(train_x)//35,150, 'normal','softmax')

#Part 1

import pickle

#Train models
model_relu.fit(train_x,train_y,valid_x,valid_y)
model_leakyrelu.fit(train_x,train_y,valid_x,valid_y)
model_sigmoid.fit(train_x,train_y,valid_x,valid_y)
model_linear.fit(train_x,train_y,valid_x,valid_y)
model_tanh.fit(train_x,train_y,valid_x,valid_y)
model_softmax.fit(train_x,train_y,valid_x,valid_y)

#Report Test Accuracy
def printacc(val,fun):
  print(f"Accuracy of {fun} is :{val}")

#function to plot losses
def dracplot(model,function_name):
  plt.plot([i for i in range(len(model.loss_in_train))],model.loss_in_train,label='Mean Training loss')
  plt.plot([i for i in range(len(model.loss_in_val))],model.loss_in_val,label='Mean Validation loss')
  plt.title(f'Loss curve for {function_name}')
  plt.xlabel('Epochs')
  plt.ylabel('Loss Value')
  plt.legend()
  plt.show()

#Print score of trained models
a1=calculate_score(model_relu.predict(test_x),test_y)
a2=calculate_score(model_leakyrelu.predict(test_x),test_y)
a3=calculate_score(model_sigmoid.predict(test_x),test_y)
a4=calculate_score(model_linear.predict(test_x),test_y)
a5=calculate_score(model_tanh.predict(test_x),test_y)
a6=calculate_score(model_softmax.predict(test_x),test_y)

#Print score
printacc(a1,"Relu")
printacc(a2,"Leakly Relu")
printacc(a3,"Sigmoid")
printacc(a4,'Linear')
printacc(a5,'tanh')
printacc(a6,'Softmax')

#Draw plot
dracplot(model_relu,"Relu")
dracplot(model_leakyrelu,"Leakly Relu")
dracplot(model_sigmoid,"Sigmoid")
dracplot(model_linear,'Linear')
dracplot(model_tanh,'tanh')
dracplot(model_softmax,'Softmax')

from sklearn.neural_network import MLPClassifier

def printscore(val,fun):
  print(f"Accuracy of {fun} is :{val}")

#Models from sklearn
mlp_model_relu=MLPClassifier(hidden_layer_sizes=(256, 128, 64,32), activation='relu',solver='sgd',batch_size=len(train_x)//35,learning_rate_init=0.08,verbose=False)
mlp_model_sigmoid=MLPClassifier(hidden_layer_sizes=(256, 128, 64,32), activation='logistic',solver='sgd',batch_size=len(train_x)//35,learning_rate_init=0.08,verbose=False)
mlp_model_linear=MLPClassifier(hidden_layer_sizes=(256, 128, 64,32), activation='identity',solver='sgd',batch_size=len(train_x)//35,learning_rate_init=0.08,verbose=False)
mlp_model_tanh=MLPClassifier(hidden_layer_sizes=(256, 128, 64,32), activation='tanh',solver='sgd',batch_size=len(train_x)//35,learning_rate_init=0.08,verbose=False)

#Train and print score of each model
mlp_model_linear.fit(train_x,train_y)
pred=mlp_model_linear.predict(test_x)
sk2=calculate_score(pred,test_y)
printscore(sk2,'Linear Function')

mlp_model_sigmoid.fit(train_x,train_y)
pred=mlp_model_sigmoid.predict(test_x)
sk1=calculate_score(pred,test_y)
printscore(sk1,'Sigmoid Function')

mlp_model_tanh.fit(train_x,train_y)
pred=mlp_model_tanh.predict(test_x)
sk3=calculate_score(pred,test_y)
printscore(sk3,'Tanh Function')

mlp_model_relu.fit(train_x,train_y)
pred=mlp_model_relu.predict(test_x)
sk4=calculate_score(pred,test_y)
printscore(sk4,'Relu Function')

#Part-5 Bonus part
import pickle
alpha=[0.001,0.01,0.1,1]

#Train and print results of model for different alpha values
for i in alpha:
  model_5=MyNeuralNetwork([784, 256, 128, 64,32, 10],i,len(train_x)//35,100, 'normal','tanh')
  model_5.fit(train_x,train_y,valid_x,valid_y)
  accuracy=calculate_score(model_5.predict(test_x),test_y)
  print(f"Accuracy using alpha= {i} is :{accuracy}")
  dracplot(model_5,f"Tanh at alpha={i}")
  with open(f'model_5_{i}.pkl','wb') as myfile:
    pickle.dump(model_5,myfile)