# -*- coding: utf-8 -*-
"""q1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Z2_nITRlJndMql8V3i-Dddz1SrKKiGsk
"""

# Commented out IPython magic to ensure Python compatibility.
from google.colab import drive
drive.mount('/content/gdrive')
# %cd '/content/gdrive/MyDrive/ML datasets'
# %ls

import pandas as pd
import math
import matplotlib.pyplot as plt
import numpy as np
from sklearn import linear_model
from sklearn.model_selection import GridSearchCV
from sklearn.linear_model import LinearRegression, Ridge
import warnings
warnings.filterwarnings('ignore')

cols=["Sex","Length","Diameter","Height","Whole weight","Shucked weight","Viscera weight","Shell weight","Rings"]
data=pd.read_csv("abalone.data",names=cols)

#Pre-processing 
print(data.describe()) #Stats about data
data['Height']=data['Height'].replace(0,data['Height'].median())  #replacing zeros with median
np.random.seed(0)
data=data.sample(frac=1) #Random Shuffling

data_x=data.drop(['Rings'],axis=1).values  #dependent variables
data_y=data['Rings'].values    #Result

print(data.isna().sum()) #No NA values

#Mapping values M=0, F=1, I=2
for i in range(data_x.shape[0]):
  if data_x[i,0]=='M':
    data_x[i,0]=0
  elif data_x[i,0]=='F':
    data_x[i,0]=1
  else:
    data_x[i,0]=2

#Spliting data using 8:2 train:test split
splitInd=int(data_x.shape[0]*(0.8))
train_x=data_x[0:splitInd,]
test_x=data_x[splitInd:,]

splitInd=int(data_y.shape[0]*(0.8))
train_y=data_y[0:splitInd,]
test_y=data_y[splitInd:,]

#Function of Hypothesis h=X*theta
def HypothesisFunction(theta,X):
  return np.dot(X,theta)

#Cost Function
def CostFunction(X,y,theta):
  m=X.shape[0]
  ycap=HypothesisFunction(theta,X)
  return (np.sum(np.square(ycap-y)))/(2*m)

#Gradient Descent
def GradinetDescent(X,y,theta,iterations,alpha):
  costs=[]  #cost list
  m=X.shape[0]  #data size

  for i in range(iterations):
    ycap=HypothesisFunction(theta,X)
    #updated theta
    theta=theta-alpha*(1/m)*(np.dot(X.T,ycap-y))
    #new cost
    newcost=CostFunction(X,y,theta)
    costs.append(newcost)
  return theta, costs

#Feature vector
theta=np.zeros(8)
iterations=10000
alpha=0.005
theta, costs=GradinetDescent(train_x,train_y,theta,iterations,alpha)
plt.scatter(x=list(range(0,iterations)),y=costs)
plt.title('Cost vs iterations plot')
plt.xlabel('Iterations')
plt.ylabel('Cost')
plt.show()

#RMSE for training
y_pred_train=np.dot(train_x,theta)
rmse_train=np.sqrt((np.sum(np.abs(y_pred_train-train_y)))*(1/train_x.shape[0]))
print("RMSE in Training: "+str(rmse_train))
#RMSE for testing
y_pred_test=np.dot(test_x,theta)
rmse_test=np.sqrt((np.sum(np.abs(y_pred_test-test_y)))*(1/test_x.shape[0]))
print("RMSE in Testing: "+str(rmse_test))
print(theta)

#10 different alpha values
alp=[1e-10, 1e-8,1e-6,1e-5,1e-4,1e-3,1e-2, 1, 5, 10]
rmseLaggo=[]
parameterl=[]
#Calculation using lasso Regression
for i in range(len(alp)):
  lasso_reg=linear_model.Lasso(alpha=alp[i],max_iter=10e5)
  lasso_reg.fit(train_x,train_y)
  y_pred_test=lasso_reg.predict(test_x)
  rmse_test=np.sqrt((np.sum(np.abs(y_pred_test-test_y)))*(1/test_x.shape[0]))
  rmseLaggo.append(rmse_test)
  parameterl.append(lasso_reg.coef_)

plt.figure(figsize=(5,5))
plt.style.use('seaborn')

#lasso Regression plot
x=range(len(alp))
plt.plot(x,rmseLaggo,color='red',label='Test',marker='^')
plt.xlabel('Index of Alpha')
plt.title('RMSE Lasso')
plt.legend()
plt.ylabel('RMSE')
plt.show()

print('\nLasso Regression best parameters are')
print(parameterl[5])
print('At Alpha: '+str(alp[5])+'\n')
rmseRidge=[]
parameterr=[]

#Calculation using Ridge Regression
for i in range(len(alp)):
  ridge_reg=linear_model.Ridge(alpha=alp[i],max_iter=10e5)
  ridge_reg.fit(train_x,train_y)
  y_pred_test=ridge_reg.predict(test_x)
  rmse_test=np.sqrt((np.sum(np.abs(y_pred_test-test_y)))*(1/test_x.shape[0]))
  rmseRidge.append(rmse_test)
  parameterr.append(ridge_reg.coef_)

#Ridge Regression plot
plt.figure(figsize=(5,5))
plt.plot(x,rmseRidge,color='blue',label='Test',marker='^')
plt.xlabel('Index of Alpha')
plt.legend()
plt.title('RMSE Ridge')
plt.ylabel('RMSE')
plt.show()
print('\nRidge Regression best parameters are')
print(parameterr[7])
print('At Alpha: '+str(alp[7])+'\n')

#Q1.2.b

GridSeachRigde = GridSearchCV(estimator= linear_model.Ridge(), param_grid=dict(alpha=alp))
GridSeachRigde.fit(train_x,train_y)
print('Grid Search Alpha and Coeficents for RIDGE')
print(GridSeachRigde.best_estimator_.alpha)
print(GridSeachRigde.best_estimator_.coef_)

print('Grid Search Alpha and Coeficents for LASSO')
GridSeachLasso = GridSearchCV(estimator= linear_model.Lasso(), param_grid=dict(alpha=alp))
GridSeachLasso.fit(train_x,train_y)
print(GridSeachLasso.best_estimator_.alpha)
print(GridSeachLasso.best_estimator_.coef_)

c1=parameterl[5]
c2=parameterr[7]
c3=GridSeachRigde.best_estimator_.coef_
c4=GridSeachLasso.best_estimator_.coef_

print("Mean Absolute difference between coeff. of Lasso",abs(c1-c4).mean())
print("Mean Absolute difference between coeff. of Rigde",abs(c2-c3).mean())