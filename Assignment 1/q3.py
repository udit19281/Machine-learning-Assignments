# -*- coding: utf-8 -*-
"""q3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1hntKL-uAXDiNqDAYWylA6DlzLSxgksZ2
"""

# Commented out IPython magic to ensure Python compatibility.
from google.colab import drive
drive.mount('/content/gdrive')
# %cd '/content/gdrive/MyDrive/ML datasets'
# %ls

import pandas as pd
import math
import random
from sklearn.metrics import confusion_matrix
import matplotlib.pyplot as plt
import numpy as np
from sklearn.metrics import confusion_matrix
from sklearn.preprocessing import Binarizer
import warnings
warnings.filterwarnings('ignore')

train=pd.read_csv('fashion-mnist_train.csv')
test=pd.read_csv('fashion-mnist_test.csv')

#Q.3.1
np.random.seed(0)

#Filter the required rows
tra=train.loc[(train['label']==1) | (train['label']==2)]
tes=test.loc[(test['label']==1) | (test['label']==2)]

#replace 2 with 0
tra['label']=tra['label'].replace(2,0)
tes['label']=tes['label'].replace(2,0)

#Random Shuffling
train=tra.sample(frac=1)
test=tes.sample(frac=1)

#Seperating training and testing data
train_x=train.drop(['label'],axis=1).values
train_y=train['label'].values
test_x=test.drop(['label'],axis=1).values
test_y=test['label'].values

#Binarizing train and test
bin=Binarizer(127)
train_x=bin.fit_transform( train_x )
test_x=bin.fit_transform( test_x )

#Class Implementing Naive Bayes
class NaiveBayes():

  def __init__(self,X):
    self.features=X.shape[1]
    self.dataSize=X.shape[0]
    self.classes=[0,1]
    self.totalClasses=len(self.classes)  #Total classes
    self.mean=np.zeros((self.totalClasses,self.features),dtype=float)
    self.variance=np.zeros((self.totalClasses,self.features),dtype=float)
    self.priorProbability=np.zeros(self.totalClasses,dtype=float)

  #Mean, Variance and Prior Probability for each class
  def __stat(self,X,y):
    i=0
    for x in self.classes:
      count=X[y==x]
      self.mean[i,:]=count.mean(axis=0)
      self.variance[i,:]=count.var(axis=0)
      self.priorProbability[i]=(count.shape[0])/float(self.dataSize)
      i=i+1
    return

  def fit(self,X,y):
    self.__stat(X,y)
  
  #Given class z what is the probality of x
  def __likelyHood(self,x,z):
    expp=(np.exp(-(x-self.mean[z])**2/(2*np.square(self.variance[z])))+1)
    deno=(np.sqrt(2*np.pi*(self.variance[z]))+2)
    return expp/deno

  #Classify a single entity
  def __classify(self,x):
    i=0
    posteriorProbs=[]
    for z in self.classes:
      likelyhood=np.sum(np.log(self.__likelyHood(x,i) ))
      priorProb=np.log(self.priorProbability[i])
      posteriorProbs.append(priorProb+likelyhood)
      i=i+1
    return self.classes[np.argmax(posteriorProbs)]

  #Predict Results
  def predict(self,X):
    ypred=[]
    for i in X:
      ypred.append(self.__classify(i))
    return ypred

  #Calculate accuracy
  def accuracy(self,y,ypred):
    r=np.sum(y==ypred)
    return r/len(y)

#Model training and testing
model=NaiveBayes(train_x)
model.fit(train_x,train_y)
pred=model.predict(test_x)
acc=model.accuracy(pred,test_y)
print("Accuracy of the model is:",acc)

#Q3.2
#Preprocessing

#Filter the required rows
tra=train.loc[(train['label']==1) | (train['label']==0)]
tes=test.loc[(test['label']==1) | (test['label']==0)]

#Joining the sample
datasample=pd.concat([tra,tes])

#Random Shuffling
datasample=datasample.sample(frac=1)

#List of columns
list_col=list(datasample.columns)
labels=datasample['label'].values

#Binarizing the whole datasample
bin=Binarizer(127)
datasample=bin.fit_transform( datasample.drop(['label'],axis=1).values )
datasample=np.insert(datasample,0,labels,axis=1)

#K fold Cross Validation Method
def crossVaidation(data,folds):
  splits=list()
  copy=list(data)
  sizeOfFolds=int(len(data)/folds)  

  for i in range(folds):
    temp=list() 
    while len(temp)<sizeOfFolds:
      i=random.randrange(len(copy))
      temp.append(copy.pop(i))
    splits.append(temp)   
  return splits

#Function to calculate accuracy for each fold
def scoreAverage(splits):
  accuracy=[]
  idx=0
  for i in splits:
    trai=[]
    trai=list(splits)
    trai.pop(idx)  
    idx=idx+1
    test=[]
    train=sum(trai,[])
    for r in i:
      rows=list(r)
      test.append(rows)
    
    train=pd.DataFrame(train,columns=list_col)
    test=pd.DataFrame(test,columns=list_col)

    #Seperating training and testing data
    train_x=train.drop(['label'],axis=1).values
    train_y=train['label'].values
    test_x=test.drop(['label'],axis=1).values
    test_y=test['label'].values

    #Model training and testing
    model=NaiveBayes(train_x)
    model.fit(train_x,train_y)
    pred=model.predict(train_x)

    #Accuracy of given fold
    acc=model.accuracy(pred,train_y)
    print("Accuracy of the Fold "+ str(idx)+" is:"+ str(acc))
    accuracy.append(acc)
  return accuracy

splits=crossVaidation(datasample,4)
score=scoreAverage(splits)

#Average Accuracy
print("Average Accuracy is: ",sum(score)/float(len(score)))

from sklearn import metrics

#Confusion Matrix
conMatrix=confusion_matrix(test_y,pred)
print("Confusion Matrix")
print(conMatrix)

#ROC Plot
x,y,z=metrics.roc_curve(test_y, pred, pos_label=1) 
plt.plot(x,y)
plt.ylabel('FPR')
plt.title("ROC Plot")
plt.xlabel('TPR')
plt.show()

TP=conMatrix[0][0]
TN=conMatrix[1][1]
FP=conMatrix[0][1]
FN=conMatrix[1][0]
accuracy=(TP+TN)/(TP+TN+FP+FN)
precision=TP/(TP+FP)
recall=TP/(TP+FN)
flScore=2 * ((precision * recall)/(precision + recall))
print('Accuracy is: '+str(accuracy))
print('Precision is: '+str(precision))
print('Recall is: '+str(recall))
print('F1 Score is: '+str(flScore))