# -*- coding: utf-8 -*-
"""q2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1YknW8wbQM7e2tVWwhz8vNlof1rh0-I5J
"""

# Commented out IPython magic to ensure Python compatibility.
from google.colab import drive
drive.mount('/content/gdrive')
# %cd '/content/gdrive/MyDrive/ML datasets'
# %ls

#Q2
import pandas as pd
import math
import matplotlib.pyplot as plt
import numpy as np
from sklearn.metrics import confusion_matrix
import warnings
warnings.filterwarnings('ignore')
data=pd.read_csv('diabetes2.csv')

# #Pre-processing
print(data.describe()) #Stats about current data

#Replacing zeroes with median
data['Pregnancies']=data['Pregnancies'].replace(0,data['Pregnancies'].median())
data['Glucose']=data['Glucose'].replace(0,data['Glucose'].median())
data['BloodPressure']=data['BloodPressure'].replace(0,data['BloodPressure'].median())
data['SkinThickness']=data['SkinThickness'].replace(0,data['SkinThickness'].median())
data['Insulin']=data['Insulin'].replace(0,data['Insulin'].median())
data['BMI']=data['BMI'].replace(0,data['BMI'].median())
print((data==0).sum(axis=0),data.shape) #Total zeroes in each column

#Normalize data using min-max normalizarion
for col in data.columns:
   data[col]=(data[col]-data[col].min())/(data[col].max()-data[col].min())
print(data.shape)

np.random.seed(0)
data=data.sample(frac=1) #Random Shuffling data

# print(data.isna().sum()) #No NA values
data=data.values

#Splitting data
m=data.shape[0]
train=data[0:int(m*0.7),:]
valid=data[int(m*(0.7)):int(m*(0.9)),:]
test=data[int(m*(0.9)):,]
# print(train.shape,valid.shape,test.shape)

train_x=train[:,:-1]
train_y=train[:,-1]

valid_x=valid[:,:-1]
valid_y=valid[:,-1]

test_x=test[:,:-1]
test_y=test[:,-1]
# print((pd.DataFrame(test_y)==0).sum())
# print(test_y.shape)

#Q2.1
class LogisticRegressionModel():
  def __init__(self,X,epochs=10000,lr=.1):

    self.lr=lr  #learning rate
    self.epochs=epochs
    self.numberOfFeatues=X.shape[1]
    self.dataSize=X.shape[0]
  
  #Sigmoid function
  def sigmoid(self,h):
    f=1/(1+np.exp(-h))
    return f

  #Loss function
  def lossFunction(self,ycap,y):
    l=np.mean(y*np.log(ycap)+(1-y)*np.log(1-ycap))
    return l*(-1)

  #Result Prediction function
  def predictResult(self,X,w):
    prediction_list=[]
    ypred=self.sigmoid(np.dot(X,w))
    prediction_list=ypred>0.5
    return prediction_list

  #Train using Batch Gradient Descent
  def trainBGD(self,X,y):
    weights=np.zeros(self.numberOfFeatues)
    training_loss=[]
    validation_loss=[]
    for i in range(self.epochs):

      #Y predicated
      ycap=self.sigmoid(np.dot(X,weights)) 

      #Weight adjust
      weights=weights- self.lr*(2/self.dataSize)*(np.dot(X.T,ycap-y))

      #Training loss calculation
      loss=self.lossFunction(ycap,y)            
      training_loss.append(loss)                

      #Validation Loss calculation
      ycap2=self.sigmoid(np.dot(valid_x,weights))
      validation_loss.append(self.lossFunction(ycap2,valid_y))

    return weights, training_loss, validation_loss

  #Train using Stochastic Gradient Descent
  def trainSGD(self,X,y):
    weights=np.zeros(self.numberOfFeatues)
    training_loss=[]
    validation_loss=[]
    for i in range(self.epochs):

      #Choose a random sample in each iteration
      random_index=np.random.randint(0,self.dataSize-1)
      x_sample=X[random_index]
      y_sample=y[random_index]

      #Y predicated
      ycap=self.sigmoid(np.dot(x_sample,weights)) 

      #Weight adjust
      weights=weights- self.lr*(2/self.dataSize)*(np.dot(x_sample.T,ycap-y_sample))

      #Training loss calculation
      loss=self.lossFunction(ycap,y_sample)            
      training_loss.append(loss)                

      #Validation Loss calculation
      ycap2=self.sigmoid(np.dot(valid_x,weights))
      validation_loss.append(self.lossFunction(ycap2,valid_y))
    return weights, training_loss, validation_loss

#Define epochs
epochs=100

lrmodel=LogisticRegressionModel(train_x,epochs=epochs)
w, training_loss, validation_loss=lrmodel.trainBGD(train_x,train_y)
y_pred=lrmodel.predictResult(train_x,w) #predicted results

#Q2.1.a
plt.scatter(x=list(range(0,epochs)),y=training_loss,color='blue',label='Training')
plt.scatter(x=list(range(0,epochs)),y=validation_loss,color='red',label='Validation')
plt.xlabel('Epochs')
plt.ylabel('Loss Value')
plt.title('Plot for BGD')
plt.legend()
plt.show()
#The stopping spot is at around 5000 iterations, after that the validation error is higher than training error

lrmodel_sgd=LogisticRegressionModel(train_x,epochs=epochs)
w_sgd, training_loss_sgd, validation_loss_sgd=lrmodel_sgd.trainSGD(train_x,train_y)
y_pred_sgd=lrmodel_sgd.predictResult(train_x,w_sgd)

plt.scatter(x=list(range(0,epochs)),y=training_loss_sgd,color='blue',label='Training')
plt.scatter(x=list(range(0,epochs)),y=validation_loss_sgd,color='red',label='Validation')
plt.xlabel('Epochs')
plt.ylabel('Loss Value')
plt.title('Plot for SGD')
plt.legend()
plt.show()

#Q2.1.b
alpha=[0.01,0.0001,10]
epochs=1000
for i in alpha:
  print('\n'+'*'*10+'Plot For Alpha: '+str(i)+'*'*10+'\n')

  lrmodel_2=LogisticRegressionModel(train_x,lr=i,epochs=epochs)
  w_2, training_loss_2, validation_loss_2=lrmodel_2.trainBGD(train_x,train_y)
  y_pred_2=lrmodel_2.predictResult(train_x,w)

  plt.figure(figsize=(5,5))
  plt.scatter(x=list(range(0,epochs)),y=training_loss_2,color='blue',label='Training')
  plt.scatter(x=list(range(0,epochs)),y=validation_loss_2,color='red',label='Validation')
  plt.xlabel('Epochs')
  plt.ylabel('Loss Value')
  plt.title('Plot for BGD')
  plt.legend()
  plt.show()

  
  lrmodel_2_sgd=LogisticRegressionModel(train_x,lr=i,epochs=epochs)
  w_2_sgd, training_loss_2_sgd, validation_loss_2_sgd=lrmodel_2_sgd.trainSGD(train_x,train_y)
  y_pred_2_sgd=lrmodel_2_sgd.predictResult(train_x,w_sgd)

  plt.figure(figsize=(5,5))
  plt.scatter(x=list(range(0,epochs)),y=training_loss_2_sgd,color='blue',label='Training')
  plt.scatter(x=list(range(0,epochs)),y=validation_loss_2_sgd,color='red',label='Validation')
  plt.xlabel('Epochs')
  plt.ylabel('Loss Value')
  plt.title('Plot for SGD')
  plt.legend()
  plt.show()

#Q2.1.c
#Y Predicted
y_pred=lrmodel_sgd.predictResult(test_x,w_sgd)

#Confussion Matrix
conMatrix=confusion_matrix(test_y,y_pred)
print("Confusion Matrix")
print(conMatrix)

TP=conMatrix[0][0]
TN=conMatrix[1][1]
FP=conMatrix[0][1]
FN=conMatrix[1][0]
accuracy=(TP+TN)/(TP+TN+FP+FN)
precision=TP/(TP+FP)
recall=TP/(TP+FN)
flScore=2 * ((precision * recall)/(precision + recall))
print('Accuracy is: '+str(accuracy))
print('Precision is: '+str(precision))
print('Recall is: '+str(recall))
print('F1 Score is: '+str(flScore))

#Q2.2
from io import StringIO
import sys
from sklearn.linear_model import LogisticRegression,SGDClassifier
prev_out=sys.stdout
sys.stdout=myout=StringIO()
#Choosing 
model=SGDClassifier(max_iter=100,alpha=0.1,loss='log',verbose=1,
                  n_iter_no_change=100,fit_intercept=False,random_state=0)
model.fit(train_x,train_y)
loss_hist=myout.getvalue()
sys.stdout=prev_out
loss_list=[]
#Q2.2.a
for line in loss_hist.split('\n'):
    if(len(line.split("loss: ")) == 1):
        continue
    loss_list.append(float(line.split("loss: ")[-1]))
plt.plot(list(range(len(loss_list))), loss_list,color='red',label='Sklearn Implementation')
plt.plot(list(range(len(loss_list))),training_loss_sgd,color='blue',label='My Implementation')
plt.xlabel("Epochs"); plt.ylabel("Loss")
plt.title('Comparison between Loss Functions of SGD')
plt.legend()
plt.show()
print('4')

#Sklearn epoches=60 to 80, my 70

#Q2.2.c
print('\n'+'*'*10+'Sklearn Performance'+'*'*10+'\n')
my_pred=model.predict(test_x)
my_pred=my_pred>0.5
conMatrix1=confusion_matrix(test_y,my_pred)
print("Confusion Matrix")
print(conMatrix1)
TP=conMatrix1[0][0]
TN=conMatrix1[1][1]
FP=conMatrix1[0][1]
FN=conMatrix1[1][0]
accuracy1=(TP+TN)/(TP+TN+FP+FN)
precision1=TP/(TP+FP)
recall1=TP/(TP+FN)
flScore1=2 * ((precision1 * recall1)/(precision1 + recall1))
print('Accuracy is: '+str(accuracy1))
print('Precision is: '+str(precision1))
print('Recall is: '+str(recall1))
print('F1 Score is: '+str(flScore1))
print('\n'+'*'*10+'My Performance'+'*'*10+'\n')
print('Accuracy is: '+str(accuracy))
print('Precision is: '+str(precision))
print('Recall is: '+str(recall))
print('F1 Score is: '+str(flScore))